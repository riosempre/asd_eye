{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "mjoi4T4H0qoZ",
        "outputId": "4e30d384-042f-4a6e-d337-448992611761"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "xay0ckHD0m3E"
      },
      "outputs": [],
      "source": [
        "#options: cc200, dosenbach160, aal\n",
        "p_ROI = \"cc200\"\n",
        "p_fold = 10\n",
        "p_center = \"Stanford\"\n",
        "p_mode = \"whole\"\n",
        "#p_mode = \"percenter\"\n",
        "p_augmentation = False\n",
        "p_Method = \"ASD-DiagNet\"\n",
        "#p_Method = 'SVM'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "m2RwElhZ0m3I",
        "outputId": "aa8d2dea-119b-4f40-9cf6-9f5b72b15eab"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "*****List of patameters****\n",
            "ROI atlas:  cc200\n",
            "per Center or whole:  whole\n",
            "Method's name:  ASD-DiagNet\n",
            "Augmentation:  False\n"
          ]
        }
      ],
      "source": [
        "parameter_list = [p_ROI,p_fold,p_center,p_mode,p_augmentation,p_Method]\n",
        "print(\"*****List of patameters****\")\n",
        "print(\"ROI atlas: \",p_ROI)\n",
        "print(\"per Center or whole: \",p_mode)\n",
        "if p_mode == 'percenter':\n",
        "    print(\"Center's name: \",p_center)\n",
        "print(\"Method's name: \",p_Method)\n",
        "if p_Method == \"ASD-DiagNet\":\n",
        "    print(\"Augmentation: \",p_augmentation)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "x-WcqAgQ0m3J",
        "outputId": "2ee72150-fd09-4977-b0a0-77b5120dceca"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pyprind\n",
            "  Downloading PyPrind-2.11.3-py2.py3-none-any.whl (8.4 kB)\n",
            "Installing collected packages: pyprind\n",
            "Successfully installed pyprind-2.11.3\n"
          ]
        }
      ],
      "source": [
        "!pip install pyprind\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "from functools import reduce\n",
        "from sklearn.impute import SimpleImputer\n",
        "import time\n",
        "\n",
        "\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "import torch\n",
        "import pyprind\n",
        "import sys\n",
        "import pickle\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from sklearn.model_selection import KFold, StratifiedKFold\n",
        "import torch.optim as optim\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from scipy import stats\n",
        "from sklearn import tree\n",
        "import functools\n",
        "import numpy.ma as ma # for masked arrays\n",
        "import pyprind\n",
        "import random\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import scipy\n",
        "import scipy.io"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ijBx6Fzk0m3K"
      },
      "source": [
        "## Importing the data "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "vfmZJCPf0m3L"
      },
      "outputs": [],
      "source": [
        "def get_key(filename):\n",
        "    f_split = filename.split('_')\n",
        "    if f_split[3] == 'rois':\n",
        "        key = '_'.join(f_split[0:3]) \n",
        "    else:\n",
        "        key = '_'.join(f_split[0:2])\n",
        "    return key\n",
        "\n",
        "def to_triangle(inp_path):\n",
        "    mat = scipy.io.loadmat(inp_path)\n",
        "    corr_mat = mat['Z']\n",
        "\n",
        "    #with np.errstate(invalid=\"ignore\"):\n",
        "    with np.errstate(invalid=\"raise\"):\n",
        "        corr = np.nan_to_num(corr_mat)\n",
        "        mask = np.invert(np.tri(corr.shape[0], k=-1, dtype=bool))\n",
        "        m = ma.masked_where(mask == 1, mask)\n",
        "        return (ma.masked_where(m, corr).compressed(), corr)\n",
        "\n",
        "def confusion(g_turth,predictions):\n",
        "    tn, fp, fn, tp = confusion_matrix(g_turth,predictions).ravel()\n",
        "    accuracy = (tp+tn)/(tp+fp+tn+fn)\n",
        "    sensitivity = (tp)/(tp+fn)\n",
        "    specificty = (tn)/(tn+fp)\n",
        "    return accuracy,sensitivity,specificty\n",
        "#отделяем один конкретный регион\n",
        "def get_regs(samplesnames,regnum):\n",
        "    datas = []\n",
        "    for sn in samplesnames:\n",
        "        datas.append(all_corr[sn][0])\n",
        "    datas = np.array(datas)\n",
        "    avg=[]\n",
        "    print(datas.shape)\n",
        "    for ie in range(datas.shape[1]):\n",
        "        avg.append(np.mean(datas[:,ie]))\n",
        "    avg=np.array(avg)\n",
        "    highs=avg.argsort()[-regnum:][::-1]\n",
        "    lows=avg.argsort()[:regnum][::-1]\n",
        "    regions=np.concatenate((highs,lows),axis=0)\n",
        "    return regions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "fgoeKoz10m3M"
      },
      "outputs": [],
      "source": [
        "directory = '/content/drive/My Drive/asd_eye_project/roi_data_new'\n",
        "\n",
        "all_corr = {}\n",
        "all_matrix = {}\n",
        "for folder in os.walk(directory):\n",
        "  if folder[2]:\n",
        "    folder_name = folder[0].split('/')[-1]\n",
        "    if folder_name[:4] == 'open':\n",
        "      lab = 1\n",
        "    else:\n",
        "      lab = 0\n",
        "    for folder_file in folder[2]:\n",
        "      if 'resultsROI_Subject' in folder_file:\n",
        "        subject_name = folder_name + '_' + folder_file[11:21]\n",
        "        file_path = folder[0] + '/' + folder_file\n",
        "        triangle_matrix = to_triangle(file_path)\n",
        "        all_corr[subject_name] = (triangle_matrix[0], lab)\n",
        "        all_matrix[subject_name] = triangle_matrix[1]\n",
        "flist = np.array(list(all_corr.keys()))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D9hbt2a00m3N"
      },
      "source": [
        "### Helper functions for computing correlations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "2a51FWNU0m3O",
        "outputId": "7e06a637-973d-4426-acea-ac355925b885"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array(['closed_01_115_Subject001', 'closed_01_115_Subject002',\n",
              "       'closed_01_115_Subject003', 'closed_01_115_Subject004',\n",
              "       'closed_01_115_Subject005', 'closed_01_115_Subject008',\n",
              "       'closed_01_115_Subject009', 'closed_01_115_Subject007',\n",
              "       'closed_01_115_Subject006', 'closed_01_115_Subject010',\n",
              "       'closed_01_115_Subject011', 'closed_01_115_Subject012',\n",
              "       'closed_01_115_Subject014', 'closed_01_115_Subject015',\n",
              "       'closed_01_115_Subject013', 'closed_01_115_Subject016',\n",
              "       'closed_01_115_Subject017', 'closed_01_115_Subject018',\n",
              "       'closed_01_115_Subject021', 'closed_01_115_Subject019',\n",
              "       'closed_01_115_Subject020', 'closed_01_115_Subject023',\n",
              "       'closed_01_115_Subject026', 'closed_01_115_Subject022',\n",
              "       'closed_01_115_Subject024', 'closed_01_115_Subject025',\n",
              "       'closed_01_115_Subject032', 'closed_01_115_Subject031',\n",
              "       'closed_01_115_Subject029', 'closed_01_115_Subject028',\n",
              "       'closed_01_115_Subject030', 'closed_01_115_Subject027',\n",
              "       'closed_01_115_Subject037', 'closed_01_115_Subject033',\n",
              "       'closed_01_115_Subject036', 'closed_01_115_Subject034',\n",
              "       'closed_01_115_Subject035', 'closed_01_115_Subject038',\n",
              "       'closed_01_115_Subject040', 'closed_01_115_Subject041',\n",
              "       'closed_01_115_Subject039', 'closed_01_115_Subject042',\n",
              "       'closed_01_115_Subject043', 'closed_01_115_Subject048',\n",
              "       'closed_01_115_Subject045', 'closed_01_115_Subject047',\n",
              "       'closed_01_115_Subject046', 'closed_01_115_Subject044',\n",
              "       'closed_01_115_Subject053', 'closed_01_115_Subject051',\n",
              "       'closed_01_115_Subject052', 'closed_01_115_Subject050',\n",
              "       'closed_01_115_Subject049', 'closed_01_115_Subject056',\n",
              "       'closed_01_115_Subject054', 'closed_01_115_Subject058',\n",
              "       'closed_01_115_Subject057', 'closed_01_115_Subject055',\n",
              "       'closed_01_115_Subject060', 'closed_01_115_Subject064',\n",
              "       'closed_01_115_Subject063', 'closed_01_115_Subject061',\n",
              "       'closed_01_115_Subject062', 'closed_01_115_Subject059',\n",
              "       'closed_01_115_Subject067', 'closed_01_115_Subject066',\n",
              "       'closed_01_115_Subject068', 'closed_01_115_Subject069',\n",
              "       'closed_01_115_Subject065', 'closed_01_115_Subject074',\n",
              "       'closed_01_115_Subject073', 'closed_01_115_Subject070',\n",
              "       'closed_01_115_Subject072', 'closed_01_115_Subject071',\n",
              "       'closed_01_115_Subject076', 'closed_01_115_Subject078',\n",
              "       'closed_01_115_Subject077', 'closed_01_115_Subject079',\n",
              "       'closed_01_115_Subject075', 'closed_01_115_Subject083',\n",
              "       'closed_01_115_Subject082', 'closed_01_115_Subject085',\n",
              "       'closed_01_115_Subject080', 'closed_01_115_Subject081',\n",
              "       'closed_01_115_Subject084', 'closed_01_115_Subject090',\n",
              "       'closed_01_115_Subject087', 'closed_01_115_Subject088',\n",
              "       'closed_01_115_Subject089', 'closed_01_115_Subject086',\n",
              "       'closed_01_115_Subject095', 'closed_01_115_Subject094',\n",
              "       'closed_01_115_Subject093', 'closed_01_115_Subject091',\n",
              "       'closed_01_115_Subject092', 'closed_01_115_Subject096',\n",
              "       'closed_01_115_Subject097', 'closed_01_115_Subject098',\n",
              "       'closed_01_115_Subject099', 'closed_01_115_Subject100',\n",
              "       'closed_01_115_Subject102', 'closed_01_115_Subject101',\n",
              "       'closed_01_115_Subject104', 'closed_01_115_Subject103',\n",
              "       'closed_01_115_Subject105', 'closed_01_115_Subject107',\n",
              "       'closed_01_115_Subject106', 'closed_01_115_Subject108',\n",
              "       'closed_01_115_Subject109', 'closed_01_115_Subject111',\n",
              "       'closed_01_115_Subject110', 'closed_01_115_Subject112',\n",
              "       'closed_01_115_Subject113', 'closed_01_115_Subject115',\n",
              "       'closed_01_115_Subject114', 'open_01_115_Subject117',\n",
              "       'open_01_115_Subject121', 'open_01_115_Subject116',\n",
              "       'open_01_115_Subject119', 'open_01_115_Subject120',\n",
              "       'open_01_115_Subject118', 'open_01_115_Subject122',\n",
              "       'open_01_115_Subject123', 'open_01_115_Subject124',\n",
              "       'open_01_115_Subject125', 'open_01_115_Subject126',\n",
              "       'open_01_115_Subject127', 'open_01_115_Subject131',\n",
              "       'open_01_115_Subject129', 'open_01_115_Subject130',\n",
              "       'open_01_115_Subject128', 'open_01_115_Subject132',\n",
              "       'open_01_115_Subject134', 'open_01_115_Subject136',\n",
              "       'open_01_115_Subject133', 'open_01_115_Subject135',\n",
              "       'open_01_115_Subject140', 'open_01_115_Subject137',\n",
              "       'open_01_115_Subject141', 'open_01_115_Subject142',\n",
              "       'open_01_115_Subject139', 'open_01_115_Subject138',\n",
              "       'open_01_115_Subject145', 'open_01_115_Subject144',\n",
              "       'open_01_115_Subject146', 'open_01_115_Subject147',\n",
              "       'open_01_115_Subject143', 'open_01_115_Subject151',\n",
              "       'open_01_115_Subject149', 'open_01_115_Subject150',\n",
              "       'open_01_115_Subject152', 'open_01_115_Subject148',\n",
              "       'open_01_115_Subject157', 'open_01_115_Subject155',\n",
              "       'open_01_115_Subject153', 'open_01_115_Subject156',\n",
              "       'open_01_115_Subject154', 'open_01_115_Subject158',\n",
              "       'open_01_115_Subject161', 'open_01_115_Subject162',\n",
              "       'open_01_115_Subject160', 'open_01_115_Subject159',\n",
              "       'open_01_115_Subject163', 'open_01_115_Subject166',\n",
              "       'open_01_115_Subject164', 'open_01_115_Subject168',\n",
              "       'open_01_115_Subject165', 'open_01_115_Subject167',\n",
              "       'open_01_115_Subject169', 'open_01_115_Subject170',\n",
              "       'open_01_115_Subject172', 'open_01_115_Subject171',\n",
              "       'open_01_115_Subject173', 'open_01_115_Subject177',\n",
              "       'open_01_115_Subject174', 'open_01_115_Subject176',\n",
              "       'open_01_115_Subject175', 'open_01_115_Subject178',\n",
              "       'open_01_115_Subject183', 'open_01_115_Subject180',\n",
              "       'open_01_115_Subject179', 'open_01_115_Subject181',\n",
              "       'open_01_115_Subject182', 'open_01_115_Subject186',\n",
              "       'open_01_115_Subject188', 'open_01_115_Subject184',\n",
              "       'open_01_115_Subject185', 'open_01_115_Subject187',\n",
              "       'open_01_115_Subject191', 'open_01_115_Subject193',\n",
              "       'open_01_115_Subject192', 'open_01_115_Subject190',\n",
              "       'open_01_115_Subject189', 'open_01_115_Subject194',\n",
              "       'open_01_115_Subject197', 'open_01_115_Subject198',\n",
              "       'open_01_115_Subject196', 'open_01_115_Subject195',\n",
              "       'open_01_115_Subject199', 'open_01_115_Subject200',\n",
              "       'open_01_115_Subject203', 'open_01_115_Subject201',\n",
              "       'open_01_115_Subject202', 'open_01_115_Subject208',\n",
              "       'open_01_115_Subject204', 'open_01_115_Subject205',\n",
              "       'open_01_115_Subject209', 'open_01_115_Subject206',\n",
              "       'open_01_115_Subject207', 'open_01_115_Subject211',\n",
              "       'open_01_115_Subject210', 'open_01_115_Subject212',\n",
              "       'open_01_115_Subject214', 'open_01_115_Subject213',\n",
              "       'open_01_115_Subject215', 'open_01_115_Subject216',\n",
              "       'open_01_115_Subject217', 'open_01_115_Subject219',\n",
              "       'open_01_115_Subject218', 'open_01_115_Subject220',\n",
              "       'open_01_115_Subject221', 'open_01_115_Subject222',\n",
              "       'open_01_115_Subject223', 'open_01_115_Subject224',\n",
              "       'open_01_115_Subject225', 'open_01_115_Subject226',\n",
              "       'open_01_115_Subject228', 'open_01_115_Subject227',\n",
              "       'open_01_115_Subject229', 'open_01_115_Subject230'], dtype='<U24')"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "flist\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xy3A8wiy0m3Q"
      },
      "source": [
        "## Helper fnuctions for computing correlations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "omRu2ZFS0m3R"
      },
      "outputs": [],
      "source": [
        "#all_matrix['closed_01_47_Subject046'].shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sTLzkPXK0m3S"
      },
      "source": [
        "## Computing eigenvalues and eigenvector"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "mU8u57X9YF64"
      },
      "outputs": [],
      "source": [
        "#"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "kXb6K21U0m3S",
        "outputId": "c0fba50d-a17b-4533-d7d4-82adfb5c37c1"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "0% [##############################] 100% | ETA: 00:00:00\n",
            "Total time elapsed: 00:00:09\n"
          ]
        }
      ],
      "source": [
        "if p_Method==\"ASD-DiagNet\":\n",
        "    eig_data = {}\n",
        "    pbar = pyprind.ProgBar(len(flist))\n",
        "    for f in flist:  \n",
        "        d = all_matrix[f]\n",
        "        eig_vals, eig_vecs = np.linalg.eig(d)\n",
        "\n",
        "        for ev in eig_vecs.T:\n",
        "            np.testing.assert_array_almost_equal(1.0, np.linalg.norm(ev))\n",
        "\n",
        "        sum_eigvals = np.sum(np.abs(eig_vals))\n",
        "        # Make a list of (eigenvalue, eigenvector, norm_eigval) tuples\n",
        "        eig_pairs = [(np.abs(eig_vals[i]), eig_vecs[:,i], np.abs(eig_vals[i])/sum_eigvals)\n",
        "                     for i in range(len(eig_vals))]\n",
        "\n",
        "        # Sort the (eigenvalue, eigenvector) tuples from high to low\n",
        "        eig_pairs.sort(key=lambda x: x[0], reverse=True)\n",
        "\n",
        "        eig_data[f] = {'eigvals':np.array([ep[0] for ep in eig_pairs]),\n",
        "                       'norm-eigvals':np.array([ep[2] for ep in eig_pairs]),\n",
        "                       'eigvecs':[ep[1] for ep in eig_pairs]}\n",
        "        pbar.update()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w2uNquQG0m3S"
      },
      "source": [
        "## Calculating Eros similarity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Vc5bU5kz0m3T"
      },
      "outputs": [],
      "source": [
        "def norm_weights(sub_flist):\n",
        "    num_dim = len(eig_data[flist[0]]['eigvals'])\n",
        "    norm_weights = np.zeros(shape=num_dim)\n",
        "    for f in sub_flist:\n",
        "        norm_weights += eig_data[f]['norm-eigvals'] \n",
        "    return norm_weights\n",
        "\n",
        "def cal_similarity(d1, d2, weights, lim=None):\n",
        "    res = 0.0\n",
        "    if lim is None:\n",
        "        weights_arr = weights.copy()\n",
        "    else:\n",
        "        weights_arr = weights[:lim].copy()\n",
        "        weights_arr /= np.sum(weights_arr)\n",
        "    for i,w in enumerate(weights_arr):\n",
        "        res += w*np.inner(d1[i], d2[i])\n",
        "    return res"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wblxL5jY0m3T"
      },
      "source": [
        "## Defining dataset class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "w1HuiB910m3T"
      },
      "outputs": [],
      "source": [
        "class CC200Dataset(Dataset):\n",
        "    def __init__(self, pkl_filename=None, data=None, samples_list=None, \n",
        "                 augmentation=False, aug_factor=1, num_neighbs=5,\n",
        "                 eig_data=None, similarity_fn=None, verbose=False,regs=None):\n",
        "        self.regs=regs\n",
        "        if pkl_filename is not None:\n",
        "            if verbose:\n",
        "                print ('Loading ..!', end=' ')\n",
        "            self.data = pickle.load(open(pkl_filename, 'rb'))\n",
        "        elif data is not None:\n",
        "            self.data = data.copy()\n",
        "            \n",
        "        else:\n",
        "            sys.stderr.write('Eigther PKL file or data is needed!')\n",
        "            return \n",
        "\n",
        "        #if verbose:\n",
        "        #    print ('Preprocess..!', end='  ')\n",
        "        if samples_list is None:\n",
        "            self.flist = [f for f in self.data]\n",
        "        else:\n",
        "            self.flist = [f for f in samples_list]\n",
        "        self.labels = np.array([self.data[f][1] for f in self.flist])\n",
        "        \n",
        "        current_flist = np.array(self.flist.copy())\n",
        "        current_lab0_flist = current_flist[self.labels == 0]\n",
        "        current_lab1_flist = current_flist[self.labels == 1]\n",
        "        #if verbose:\n",
        "        #    print(' Num Positive : ', len(current_lab1_flist), end=' ')\n",
        "        #    print(' Num Negative : ', len(current_lab0_flist), end=' ')\n",
        "        \n",
        "        \n",
        "        if augmentation:\n",
        "            self.num_data = aug_factor * len(self.flist)\n",
        "            self.neighbors = {}\n",
        "            pbar = pyprind.ProgBar(len(self.flist))\n",
        "            weights = norm_weights(samples_list)#??\n",
        "            for f in self.flist:\n",
        "                label = self.data[f][1]\n",
        "                candidates = (set(current_lab0_flist) if label == 0 else set(current_lab1_flist))\n",
        "                candidates.remove(f)\n",
        "                eig_f = eig_data[f]['eigvecs']\n",
        "                sim_list = []\n",
        "                for cand in candidates:\n",
        "                    eig_cand = eig_data[cand]['eigvecs']\n",
        "                    sim = similarity_fn(eig_f, eig_cand,weights)\n",
        "                    sim_list.append((sim, cand))\n",
        "                sim_list.sort(key=lambda x: x[0], reverse=True)\n",
        "                self.neighbors[f] = [item[1] for item in sim_list[:num_neighbs]]#list(candidates)#[item[1] for item in sim_list[:num_neighbs]]\n",
        "        \n",
        "        else:\n",
        "            self.num_data = len(self.flist)\n",
        "\n",
        "        \n",
        "    def __getitem__(self, index):\n",
        "        if index < len(self.flist):\n",
        "            fname = self.flist[index]\n",
        "            data = self.data[fname][0].copy() #get_corr_data(fname, mode=cal_mode)    \n",
        "            data = data[self.regs].copy()\n",
        "            label = (self.labels[index],)\n",
        "            return torch.FloatTensor(data), torch.FloatTensor(label)\n",
        "        else:\n",
        "            f1 = self.flist[index % len(self.flist)]\n",
        "            d1, y1 = self.data[f1][0], self.data[f1][1]\n",
        "            d1=d1[self.regs]\n",
        "            f2 = np.random.choice(self.neighbors[f1])\n",
        "            d2, y2 = self.data[f2][0], self.data[f2][1]\n",
        "            d2=d2[self.regs]\n",
        "            assert y1 == y2\n",
        "            r = np.random.uniform(low=0, high=1)\n",
        "            label = (y1,)\n",
        "            data = r*d1 + (1-r)*d2\n",
        "            return torch.FloatTensor(data), torch.FloatTensor(label)\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.num_data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sPVo2JO10m3U"
      },
      "source": [
        "## Definig data loader function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Dmmv6GU40m3U"
      },
      "outputs": [],
      "source": [
        "def get_loader(pkl_filename=None, data=None, samples_list=None,\n",
        "               batch_size=64, \n",
        "               num_workers=1, mode='train',\n",
        "               *, augmentation=False, aug_factor=1, num_neighbs=5,\n",
        "                 eig_data=None, similarity_fn=None, verbose=False,regions=None):\n",
        "    \"\"\"Build and return data loader.\"\"\"\n",
        "    if mode == 'train':\n",
        "        shuffle = True\n",
        "    else:\n",
        "        shuffle = False\n",
        "        augmentation=False\n",
        "\n",
        "    dataset = CC200Dataset(pkl_filename=pkl_filename, data=data, samples_list=samples_list,\n",
        "                           augmentation=augmentation, aug_factor=aug_factor, \n",
        "                           eig_data=eig_data, similarity_fn=similarity_fn, verbose=verbose,regs=regions)\n",
        "\n",
        "    data_loader = DataLoader(dataset,\n",
        "                             batch_size=batch_size,\n",
        "                             shuffle=shuffle,\n",
        "                             num_workers=num_workers)\n",
        "  \n",
        "    return data_loader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-6ie3Pj90m3V"
      },
      "source": [
        "## Defining Autoencoder class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "4iKMeoGk0m3V",
        "outputId": "eb452197-03d9-4f80-8404-c498f3fef0e8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "MTAutoEncoder(\n",
              "  (fc_encoder): Linear(in_features=990, out_features=200, bias=True)\n",
              "  (classifier): Sequential(\n",
              "    (0): Linear(in_features=200, out_features=1, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "class MTAutoEncoder(nn.Module):\n",
        "    def __init__(self, num_inputs=990, \n",
        "                 num_latent=200, tied=True,\n",
        "                 num_classes=2, use_dropout=False):\n",
        "        super(MTAutoEncoder, self).__init__()\n",
        "        self.tied = tied\n",
        "        self.num_latent = num_latent\n",
        "        \n",
        "        self.fc_encoder = nn.Linear(num_inputs, num_latent)\n",
        "    \n",
        "        if not tied:\n",
        "            self.fc_decoder = nn.Linear(num_latent, num_inputs)\n",
        "         \n",
        "        self.fc_encoder = nn.Linear(num_inputs, num_latent)\n",
        "        \n",
        "        if use_dropout:\n",
        "            self.classifier = nn.Sequential (\n",
        "                nn.Dropout(p=0.5),\n",
        "                nn.Linear(self.num_latent, 1),\n",
        "                \n",
        "            )\n",
        "        else:\n",
        "            self.classifier = nn.Sequential (\n",
        "                nn.Linear(self.num_latent, 1),\n",
        "            )\n",
        "            \n",
        "         \n",
        "    def forward(self, x, eval_classifier=False):\n",
        "        x = self.fc_encoder(x)\n",
        "        x = torch.tanh(x)\n",
        "        if eval_classifier:\n",
        "            x_logit = self.classifier(x)\n",
        "        else:\n",
        "            x_logit = None\n",
        "        \n",
        "        if self.tied:\n",
        "            x = F.linear(x, self.fc_encoder.weight.t())\n",
        "        else:\n",
        "            x = self.fc_decoder(x)\n",
        "            \n",
        "        return x, x_logit\n",
        "\n",
        "mtae = MTAutoEncoder()\n",
        "\n",
        "mtae"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ykTAn7nL0m3V"
      },
      "source": [
        "## Defining training and testing functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Pv0WYdam0m3W"
      },
      "outputs": [],
      "source": [
        "def train(model, epoch, train_loader, p_bernoulli=None, mode='both', lam_factor=1.0, optimizer = None, criterion_ae = None, criterion_clf = None):\n",
        "    model.train()\n",
        "    train_losses = []\n",
        "    for i,(batch_x,batch_y) in enumerate(train_loader):\n",
        "        if len(batch_x) != batch_size:\n",
        "            continue\n",
        "        if p_bernoulli is not None:\n",
        "            if i == 0:\n",
        "                p_tensor = torch.ones_like(batch_x).to(device)*p_bernoulli\n",
        "            rand_bernoulli = torch.bernoulli(p_tensor).to(device)\n",
        "\n",
        "        data, target = batch_x.to(device), batch_y.to(device)\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        if mode in ['both', 'ae']:\n",
        "            if p_bernoulli is not None:\n",
        "                rec_noisy, _ = model(data*rand_bernoulli, False)\n",
        "                loss_ae = criterion_ae(rec_noisy, data) / len(batch_x)\n",
        "            else:\n",
        "                rec, _ = model(data, False)\n",
        "                loss_ae = criterion_ae(rec, data) / len(batch_x)\n",
        "\n",
        "        if mode in ['both', 'clf']:\n",
        "            rec_clean, logits = model(data, True)\n",
        "            loss_clf = criterion_clf(logits, target)\n",
        "\n",
        "        if mode == 'both':\n",
        "            loss_total = loss_ae + lam_factor*loss_clf\n",
        "            train_losses.append([loss_ae.detach().cpu().numpy(), \n",
        "                                 loss_clf.detach().cpu().numpy()])\n",
        "        elif mode == 'ae':\n",
        "            loss_total = loss_ae\n",
        "            train_losses.append([loss_ae.detach().cpu().numpy(), \n",
        "                                 0.0])\n",
        "        elif mode == 'clf':\n",
        "            loss_total = loss_clf\n",
        "            train_losses.append([0.0, \n",
        "                                 loss_clf.detach().cpu().numpy()])\n",
        "\n",
        "        loss_total.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    return train_losses       \n",
        "\n",
        "def test(model, criterion, test_loader, \n",
        "         eval_classifier=False, num_batch=None, optimizer = None):\n",
        "    test_loss, n_test, correct = 0.0, 0, 0\n",
        "    all_predss=[]\n",
        "    if eval_classifier:\n",
        "        y_true, y_pred = [], []\n",
        "    with torch.no_grad():\n",
        "        model.eval()\n",
        "        for i,(batch_x,batch_y) in enumerate(test_loader, 1):\n",
        "            if num_batch is not None:\n",
        "                if i >= num_batch:\n",
        "                    continue\n",
        "            data = batch_x.to(device)\n",
        "            rec, logits = model(data, eval_classifier)\n",
        "\n",
        "            test_loss += criterion(rec, data).detach().cpu().numpy() \n",
        "            n_test += len(batch_x)\n",
        "            if eval_classifier:\n",
        "                proba = torch.sigmoid(logits).detach().cpu().numpy()\n",
        "                preds = np.ones_like(proba, dtype=np.int32)\n",
        "                preds[proba < 0.5] = 0\n",
        "                all_predss.extend(preds)###????\n",
        "                y_arr = np.array(batch_y, dtype=np.int32)\n",
        "\n",
        "                correct += np.sum(preds == y_arr)\n",
        "                y_true.extend(y_arr.tolist())\n",
        "                y_pred.extend(proba.tolist())\n",
        "        mlp_acc,mlp_sens,mlp_spef = confusion(y_true,all_predss)\n",
        "\n",
        "    return  mlp_acc,mlp_sens,mlp_spef#,correct/n_test\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "SBwMd-eS0m3W",
        "outputId": "a9f878f7-92d5-4f9a-fc95-bbac35fcb03d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cuda\n"
          ]
        }
      ],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "oNocV8P4cKps"
      },
      "outputs": [],
      "source": [
        "def test_model(directory = '/content/drive/My Drive/asd_eye_project/roi_data_new', p_Method=\"ASD-DiagNet\", num_epochs = 25, remove_percentage = 0.3, p_augmentation = False):\n",
        "  #generate data\n",
        "  all_corr = {}\n",
        "  all_matrix = {}\n",
        "  for folder in os.walk(directory):\n",
        "    if folder[2]:\n",
        "      folder_name = folder[0].split('/')[-1]\n",
        "      if folder_name[:4] == 'open':\n",
        "        lab = 1\n",
        "      else:\n",
        "        lab = 0\n",
        "      for folder_file in folder[2]:\n",
        "        if 'resultsROI_Subject' in folder_file:\n",
        "          subject_name = folder_name + '_' + folder_file[11:21]\n",
        "          file_path = folder[0] + '/' + folder_file\n",
        "          triangle_matrix = to_triangle(file_path)\n",
        "          all_corr[subject_name] = (triangle_matrix[0], lab)\n",
        "          all_matrix[subject_name] = triangle_matrix[1]\n",
        "  flist = np.array(list(all_corr.keys()))\n",
        "\t#generate eigenvalues\n",
        "  if p_Method==\"ASD-DiagNet\":\n",
        "    eig_data = {}\n",
        "    pbar = pyprind.ProgBar(len(flist))\n",
        "    for f in flist:  \n",
        "      d = all_matrix[f]\n",
        "      eig_vals, eig_vecs = np.linalg.eig(d)\n",
        "\n",
        "      for ev in eig_vecs.T:\n",
        "        np.testing.assert_array_almost_equal(1.0, np.linalg.norm(ev))\n",
        "      sum_eigvals = np.sum(np.abs(eig_vals))\n",
        "      # Make a list of (eigenvalue, eigenvector, norm_eigval) tuples\n",
        "      eig_pairs = [(np.abs(eig_vals[i]), eig_vecs[:,i], np.abs(eig_vals[i])/sum_eigvals) for i in range(len(eig_vals))]\n",
        "      # Sort the (eigenvalue, eigenvector) tuples from high to low\n",
        "      eig_pairs.sort(key=lambda x: x[0], reverse=True)\n",
        "\n",
        "      eig_data[f] = {'eigvals':np.array([ep[0] for ep in eig_pairs]),\n",
        "                    'norm-eigvals':np.array([ep[2] for ep in eig_pairs]),\n",
        "                    'eigvecs':[ep[1] for ep in eig_pairs]}\n",
        "      pbar.update()\n",
        "\n",
        "  device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "  print(device)\n",
        "\n",
        "  num_corr = len(all_corr[list(flist)[0]][0])\n",
        "  print(\"num_corr:  \",num_corr)\n",
        "\n",
        "  start =time.time()\n",
        "  batch_size = 8\n",
        "  learning_rate_ae, learning_rate_clf = 0.0001, 0.0001\n",
        "  #num_epochs = 25\n",
        "\n",
        "  p_bernoulli = None\n",
        "  augmentation = p_augmentation\n",
        "  use_dropout = False\n",
        "\n",
        "  aug_factor = 2\n",
        "  num_neighbs = 5\n",
        "  lim4sim = 2\n",
        "  n_lat = int(num_corr * remove_percentage)\n",
        "  print(n_lat)\n",
        "  start= time.time()\n",
        "\n",
        "  print('p_bernoulli: ', p_bernoulli)\n",
        "  print('augmentaiton: ', augmentation, 'aug_factor: ', aug_factor, \n",
        "      'num_neighbs: ', num_neighbs, 'lim4sim: ', lim4sim)\n",
        "  print('use_dropout: ', use_dropout, '\\n')\n",
        "\n",
        "\n",
        "  sim_function = functools.partial(cal_similarity, lim=lim4sim)\n",
        "  crossval_res_kol=[]\n",
        "  y_arr = np.array([all_corr[f][1] for f in flist])\n",
        "  kk=0 \n",
        "  for rp in range(5):\n",
        "    print(\"RP \" + str(rp))\n",
        "    kf = StratifiedKFold(n_splits=p_fold, random_state=1, shuffle=True)\n",
        "    np.random.shuffle(flist)\n",
        "    y_arr = np.array([all_corr[f][1] for f in flist])\n",
        "    for kk,(train_index, test_index) in enumerate(kf.split(flist, y_arr)):\n",
        "      print(\"Fold \" + str(kk))\n",
        "      train_samples, test_samples = flist[train_index], flist[test_index]\n",
        "\n",
        "\n",
        "      verbose = (True if (kk == 0) else False)\n",
        "\n",
        "      regions_inds = get_regs(train_samples,int(num_corr * remove_percentage))\n",
        "\n",
        "      num_inpp = len(regions_inds)\n",
        "      n_lat = int(num_inpp/2)\n",
        "      train_loader=get_loader(data=all_corr, samples_list=train_samples, \n",
        "                  batch_size=batch_size, mode='train',\n",
        "                  augmentation=augmentation, aug_factor=aug_factor, \n",
        "                  num_neighbs=num_neighbs, eig_data=eig_data, similarity_fn=sim_function, \n",
        "                  verbose=verbose,regions=regions_inds)\n",
        "\n",
        "      test_loader=get_loader(data=all_corr, samples_list=test_samples, \n",
        "                  batch_size=batch_size, mode='test', augmentation=False, \n",
        "                  verbose=verbose,regions=regions_inds)\n",
        "\n",
        "      model = MTAutoEncoder(tied=True, num_inputs=num_inpp, num_latent=n_lat, use_dropout=use_dropout)\n",
        "      model.to(device)\n",
        "      criterion_ae = nn.MSELoss(reduction='sum')\n",
        "      criterion_clf = nn.BCEWithLogitsLoss()\n",
        "      optimizer = optim.SGD([{'params': model.fc_encoder.parameters(), 'lr': learning_rate_ae},\n",
        "                  {'params': model.classifier.parameters(), 'lr': learning_rate_clf}],\n",
        "                  momentum=0.9)\n",
        "\n",
        "      for epoch in range(1, num_epochs+1):\n",
        "        print(\"Epoch \" + str(epoch))\n",
        "        if epoch <= 20:\n",
        "          train_losses = train(model, epoch, train_loader, p_bernoulli, mode='both', optimizer = optimizer, criterion_ae = criterion_ae, criterion_clf = criterion_clf)\n",
        "        else:\n",
        "          train_losses = train(model, epoch, train_loader, p_bernoulli, mode='clf', optimizer = optimizer, criterion_ae = criterion_ae, criterion_clf = criterion_clf)\n",
        "      if kk == 0:\n",
        "        print('Saving model.')\n",
        "        model_path = \"/content/drive/My Drive/asd_eye_project/model_\" + str(rp)\n",
        "        torch.save(model.state_dict(), model_path)\n",
        "      res_mlp = test(model, criterion_ae, test_loader, eval_classifier=True, optimizer = optimizer)\n",
        "      print(test(model, criterion_ae, test_loader, eval_classifier=True, optimizer = optimizer))\n",
        "      crossval_res_kol.append(res_mlp)\n",
        "    print(\"averages:\")\n",
        "    print(np.mean(np.array(crossval_res_kol),axis = 0))\n",
        "    finish= time.time()\n",
        "\n",
        "    print(finish-start)\n",
        "\n",
        "  return np.mean(np.array(crossval_res_kol),axis = 0)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AM7bh_23cPfb"
      },
      "outputs": [],
      "source": [
        "batch_size = 8\n",
        "# num_epochs = 25, remove_percentage = 0.3,    \n",
        "results = {}\n",
        "results['augment-25-30'] = test_model(p_augmentation = True)\n",
        "\n",
        "results['unaugment-25-10'] = test_model(p_augmentation = False, remove_percentage = 0.1)\n",
        "results['unaugment-25-20'] = test_model(p_augmentation = False, remove_percentage = 0.2)\n",
        "results['unaugment-25-25'] = test_model(p_augmentation = False, remove_percentage = 0.25)\n",
        "results['unaugment-25-30'] = test_model(p_augmentation = False)\n",
        "results['unaugment-25-40'] = test_model(p_augmentation = False, remove_percentage = 0.4)\n",
        "\n",
        "results['unaugment-10-30'] = test_model(p_augmentation = False, num_epochs = 10)\n",
        "results['unaugment-20-30'] = test_model(p_augmentation = False, num_epochs = 20)\n",
        "results['unaugment-25-30'] = test_model(p_augmentation = False)\n",
        "results['unaugment-30-30'] = test_model(p_augmentation = False, num_epochs = 30)\n",
        "results['unaugment-40-30'] = test_model(p_augmentation = False, num_epochs = 40)\n",
        "results['unaugment-50-30'] = test_model(p_augmentation = False, num_epochs = 50)\n",
        "print(results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bv6WawH_ujUj"
      },
      "outputs": [],
      "source": [
        "results['augment-25-30'] = test_model(p_augmentation = True)\n",
        "\n",
        "results['augment-25-10'] = test_model(p_augmentation = True, remove_percentage = 0.1)\n",
        "results['augment-25-20'] = test_model(p_augmentation = True, remove_percentage = 0.2)\n",
        "results['augment-25-25'] = test_model(p_augmentation = True, remove_percentage = 0.25)\n",
        "results['augment-25-40'] = test_model(p_augmentation = True, remove_percentage = 0.4)\n",
        "\n",
        "results['augment-10-30'] = test_model(p_augmentation = True, num_epochs = 10)\n",
        "results['augment-20-30'] = test_model(p_augmentation = True, num_epochs = 20)\n",
        "results['augment-25-30'] = test_model(p_augmentation = True)\n",
        "results['augment-30-30'] = test_model(p_augmentation = True, num_epochs = 30)\n",
        "results['augment-40-30'] = test_model(p_augmentation = True, num_epochs = 40)\n",
        "results['augment-50-30'] = test_model(p_augmentation = True, num_epochs = 50)\n",
        "\n",
        "results['unaugment-05-30'] = test_model(p_augmentation = False, num_epochs = 5)\n",
        "results['unaugment-15-30'] = test_model(p_augmentation = False, num_epochs = 15)\n",
        "results['unaugment-35-30'] = test_model(p_augmentation = False, num_epochs = 35)\n",
        "results['unaugment-45-30'] = test_model(p_augmentation = False, num_epochs = 45)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hnibrJa3GZYA"
      },
      "outputs": [],
      "source": [
        "results_keys = list(results.keys())\n",
        "results_keys.sort()\n",
        "\n",
        "result_dict = []\n",
        "for r_key in results_keys:\n",
        "  temp_dict = dict()\n",
        "  temp_dict['name'] = r_key\n",
        "  temp_dict['accuracy'] = results[r_key][0]\n",
        "  temp_dict['sensitivity'] = results[r_key][1]\n",
        "  temp_dict['specificity'] = results[r_key][2]\n",
        "  result_dict.append(temp_dict)\n",
        "result_df = pd.DataFrame.from_records(result_dict)\n",
        "\n",
        "result_df.to_csv('/content/drive/My Drive/asd_eye_project/results.csv')\n",
        "result_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qLYn-fmQY1Sd"
      },
      "outputs": [],
      "source": [
        "crush = 69 / 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hZ4fyIyv0m3X",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "\n",
        "if p_Method == \"ASD-DiagNet\" and p_mode == \"whole\":\n",
        "    \n",
        "    num_corr = len(all_corr[list(flist)[0]][0])\n",
        "    print(\"num_corr:  \",num_corr)\n",
        "    \n",
        "    start =time.time()\n",
        "    batch_size = 8\n",
        "    learning_rate_ae, learning_rate_clf = 0.0001, 0.0001\n",
        "    num_epochs = 25\n",
        "    #num_epochs = 2\n",
        "\n",
        "    p_bernoulli = None\n",
        "    augmentation = p_augmentation\n",
        "    use_dropout = False\n",
        "\n",
        "    aug_factor = 2\n",
        "    num_neighbs = 5\n",
        "    lim4sim = 2\n",
        "    n_lat = int(num_corr/4)\n",
        "    print(n_lat)\n",
        "    start= time.time()\n",
        "\n",
        "    print('p_bernoulli: ', p_bernoulli)\n",
        "    print('augmentaiton: ', augmentation, 'aug_factor: ', aug_factor, \n",
        "          'num_neighbs: ', num_neighbs, 'lim4sim: ', lim4sim)\n",
        "    print('use_dropout: ', use_dropout, '\\n')\n",
        "\n",
        "\n",
        "    sim_function = functools.partial(cal_similarity, lim=lim4sim)\n",
        "    crossval_res_kol=[]\n",
        "    y_arr = np.array([all_corr[f][1] for f in flist])\n",
        "    kk=0 \n",
        "    for rp in range(10):\n",
        "        print(\"RP \" + str(rp))\n",
        "        kf = StratifiedKFold(n_splits=p_fold, random_state=1, shuffle=True)\n",
        "        np.random.shuffle(flist)\n",
        "        y_arr = np.array([all_corr[f][1] for f in flist])\n",
        "        for kk,(train_index, test_index) in enumerate(kf.split(flist, y_arr)):\n",
        "            print(\"Fold \" + str(kk))\n",
        "            train_samples, test_samples = flist[train_index], flist[test_index]\n",
        "\n",
        "\n",
        "            verbose = (True if (kk == 0) else False)\n",
        "\n",
        "            regions_inds = get_regs(train_samples,int(num_corr * 0.3))\n",
        "\n",
        "            num_inpp = len(regions_inds)\n",
        "            n_lat = int(num_inpp/2)\n",
        "            train_loader=get_loader(data=all_corr, samples_list=train_samples, \n",
        "                                    batch_size=batch_size, mode='train',\n",
        "                                    augmentation=augmentation, aug_factor=aug_factor, \n",
        "                                    num_neighbs=num_neighbs, eig_data=eig_data, similarity_fn=sim_function, \n",
        "                                    verbose=verbose,regions=regions_inds)\n",
        "\n",
        "            test_loader=get_loader(data=all_corr, samples_list=test_samples, \n",
        "                                   batch_size=batch_size, mode='test', augmentation=False, \n",
        "                                   verbose=verbose,regions=regions_inds)\n",
        "\n",
        "            model = MTAutoEncoder(tied=True, num_inputs=num_inpp, num_latent=n_lat, use_dropout=use_dropout)\n",
        "            model.to(device)\n",
        "            criterion_ae = nn.MSELoss(reduction='sum')\n",
        "            criterion_clf = nn.BCEWithLogitsLoss()\n",
        "            optimizer = optim.SGD([{'params': model.fc_encoder.parameters(), 'lr': learning_rate_ae},\n",
        "                                   {'params': model.classifier.parameters(), 'lr': learning_rate_clf}],\n",
        "                                  momentum=0.9)\n",
        "\n",
        "            for epoch in range(1, num_epochs+1):\n",
        "                print(\"Epoch \" + str(epoch))\n",
        "                if epoch <= 20:\n",
        "                    train_losses = train(model, epoch, train_loader, p_bernoulli, mode='both')\n",
        "                else:\n",
        "                    train_losses = train(model, epoch, train_loader, p_bernoulli, mode='clf')\n",
        "            if kk == 0:\n",
        "                print('Saving model.')\n",
        "                model_path = \"/content/drive/My Drive/asd_eye_project/model_\" + str(rp)\n",
        "                torch.save(model.state_dict(), model_path)\n",
        "            res_mlp = test(model, criterion_ae, test_loader, eval_classifier=True)\n",
        "            print(test(model, criterion_ae, test_loader, eval_classifier=True))\n",
        "            crossval_res_kol.append(res_mlp)\n",
        "        print(\"averages:\")\n",
        "        print(np.mean(np.array(crossval_res_kol),axis = 0))\n",
        "        finish= time.time()\n",
        "\n",
        "        print(finish-start)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AwU01rLQ0m3X"
      },
      "outputs": [],
      "source": [
        "print('blarg')\n",
        "if p_Method == \"ASD-DiagNet\" and p_mode == \"percenter\":\n",
        "    num_corr = len(all_corr[flist[0]][0])\n",
        "\n",
        "    flist = os.listdir(data_main_path)\n",
        "\n",
        "    for f in range(len(flist)):\n",
        "        flist[f] = get_key(flist[f])\n",
        "\n",
        "    centers_dict = {}\n",
        "    for f in flist:\n",
        "        key = f.split('_')[0]\n",
        "\n",
        "        if key not in centers_dict:\n",
        "            centers_dict[key] = []\n",
        "        centers_dict[key].append(f)\n",
        "\n",
        "    \n",
        "\n",
        "    flist = np.array(centers_dict[p_center])\n",
        "    \n",
        "    start =time.time()\n",
        "    #flist = np.array(sorted(os.listdir(data_main_path)))\n",
        "    batch_size = 8\n",
        "    learning_rate_ae, learning_rate_clf = 0.0001, 0.0001\n",
        "    num_epochs = 25\n",
        "\n",
        "    p_bernoulli = None\n",
        "    augmentation = p_augmentation\n",
        "    use_dropout = False\n",
        "\n",
        "    aug_factor = 2\n",
        "    num_neighbs = 5\n",
        "    lim4sim = 2\n",
        "    n_lat = int(num_corr/4)\n",
        "\n",
        "\n",
        "    print('p_bernoulli: ', p_bernoulli)\n",
        "    print('augmentaiton: ', augmentation, 'aug_factor: ', aug_factor, \n",
        "          'num_neighbs: ', num_neighbs, 'lim4sim: ', lim4sim)\n",
        "    print('use_dropout: ', use_dropout, '\\n')\n",
        "\n",
        "\n",
        "    sim_function = functools.partial(cal_similarity, lim=lim4sim)\n",
        "    all_rp_res=[]\n",
        "    y_arr = np.array([get_label(f) for f in flist])\n",
        "\n",
        "    kk=0 \n",
        "    crossval_res_kol_kol=[]\n",
        "    for rp in range(10):\n",
        "        print(\"========================\")\n",
        "        crossval_res_kol = []\n",
        "        start= time.time()\n",
        "        kf = StratifiedKFold(n_splits=p_fold)\n",
        "        #np.random.shuffle(flist)\n",
        "        y_arr = np.array([get_label(f) for f in flist])\n",
        "        for kk,(train_index, test_index) in enumerate(kf.split(flist, y_arr)):\n",
        "        \n",
        "            train_samples, test_samples = flist[train_index], flist[test_index]\n",
        "\n",
        "            verbose = (True if (kk == 0) else False)\n",
        "\n",
        "            regions_inds = get_regs(train_samples,int(num_corr/4))\n",
        "            num_inpp = len(regions_inds)\n",
        "            n_lat = int(num_inpp/2)\n",
        "            num_inpp = len(regions_inds)\n",
        "            train_loader=get_loader(data=all_corr, samples_list=train_samples, \n",
        "                                    batch_size=batch_size, mode='train',\n",
        "                                    augmentation=augmentation, aug_factor=aug_factor, \n",
        "                                    num_neighbs=num_neighbs, eig_data=eig_data, similarity_fn=sim_function, \n",
        "                                    verbose=verbose,regions=regions_inds)\n",
        "\n",
        "            test_loader=get_loader(data=all_corr, samples_list=test_samples, \n",
        "                                   batch_size=batch_size, mode='test', augmentation=False, \n",
        "                                   verbose=verbose,regions=regions_inds)\n",
        "\n",
        "            model = MTAutoEncoder(tied=True, num_inputs=num_inpp, num_latent=n_lat, use_dropout=use_dropout)\n",
        "            model.to(device)\n",
        "            criterion_ae = nn.MSELoss(reduction='sum')\n",
        "            criterion_clf = nn.BCEWithLogitsLoss()\n",
        "            optimizer = optim.SGD([{'params': model.fc_encoder.parameters(), 'lr': learning_rate_ae},\n",
        "                                   {'params': model.classifier.parameters(), 'lr': learning_rate_clf}],\n",
        "                                  momentum=0.9)\n",
        "\n",
        "            for epoch in range(1, num_epochs+1):\n",
        "                if epoch <= 20:\n",
        "                    train_losses = train(model, epoch, train_loader, p_bernoulli, mode='both')\n",
        "                else:\n",
        "                    train_losses = train(model, epoch, train_loader, p_bernoulli, mode='clf')\n",
        "\n",
        "\n",
        "            res_mlp = test(model, criterion_ae, test_loader, eval_classifier=True)\n",
        "            #print(\"fold\",kk+1,\":\",test(model, criterion_ae, test_loader, eval_classifier=True))\n",
        "            crossval_res_kol.append(res_mlp)\n",
        "        print(\"Result of repeat \",rp,\":\")\n",
        "        print(np.mean(np.array(crossval_res_kol),axis = 0))\n",
        "        all_rp_res.append(np.mean(np.array(crossval_res_kol),axis = 0))\n",
        "        finish= time.time()\n",
        "\n",
        "        print(\"Running time:\",finish-start)\n",
        "    print(\"Avergae result of 10 repeats: \",np.mean(np.array(all_rp_res),axis = 0))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "StjGjQr30m3Y"
      },
      "outputs": [],
      "source": [
        "if p_Method != \"ASD-DiagNet\" and p_mode == \"whole\":\n",
        "    \n",
        "    clf = SVC(gamma = 'auto') if  p_Method == 'SVM' else RandomForestClassifier(n_estimators=100)\n",
        "    overall_result = []\n",
        "    for rp in range(10):\n",
        "        kf = StratifiedKFold(n_splits=p_fold, random_state=1, shuffle=True)\n",
        "        np.random.shuffle(flist)\n",
        "        y_arr = np.array([all_corr[f][1] for f in flist])\n",
        "        res = []\n",
        "        for kk,(train_index, test_index) in enumerate(kf.split(flist, y_arr)):\n",
        "            train_samples, test_samples = np.array(flist)[train_index], np.array(flist)[test_index]\n",
        "            train_data = []\n",
        "            train_labels = []\n",
        "            test_data = []\n",
        "            test_labels = []\n",
        "\n",
        "            for i in train_samples:\n",
        "                train_data.append(all_corr[i][0])\n",
        "                train_labels.append(all_corr[i][1])\n",
        "\n",
        "            for i in test_samples:\n",
        "                test_data.append(all_corr[i][0])\n",
        "                test_labels.append(all_corr[i][1])\n",
        "\n",
        "            \n",
        "            clf.fit(train_data,train_labels)\n",
        "            pr = clf.predict(test_data)\n",
        "            res.append(confusion(test_labels,pr))\n",
        "        \n",
        "        print(\"repeat: \",rp,np.mean(res, axis=0).tolist())\n",
        "        overall_result.append(np.mean(res, axis=0).tolist())   \n",
        "    print(\"---------------Result of repeating 10 times-------------------\")\n",
        "    print(np.mean(np.array(overall_result), axis=0).tolist())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ep6iDJbU0m3Y"
      },
      "outputs": [],
      "source": [
        "random.seed(19)\n",
        "np.random.seed(19)\n",
        "if p_Method != \"ASD-DiagNet\" and p_mode == \"percenter\":\n",
        "    \n",
        "    clf = SVC(gamma = 'auto') if  p_Method == 'SVM' else RandomForestClassifier(n_estimators=100)\n",
        "    overall_result = []\n",
        "    for rp in range(4,10):\n",
        "        kf = StratifiedKFold(n_splits=p_fold, random_state=1, shuffle=True)\n",
        "        np.random.shuffle(flist)\n",
        "        y_arr = np.array([all_corr[f][1] for f in flist])\n",
        "        res = []\n",
        "        for kk,(train_index, test_index) in enumerate(kf.split(flist, y_arr)):\n",
        "            train_samples, test_samples = np.array(flist)[train_index], np.array(flist)[test_index]\n",
        "            train_data = []\n",
        "            train_labels = []\n",
        "            test_data = []\n",
        "            test_labels = []\n",
        "\n",
        "            for i in train_samples:\n",
        "                train_data.append(all_corr[i][0])\n",
        "                train_labels.append(all_corr[i][1])\n",
        "\n",
        "            for i in test_samples:\n",
        "                test_data.append(all_corr[i][0])\n",
        "                test_labels.append(all_corr[i][1])\n",
        "\n",
        "            clf.fit(train_data,train_labels)\n",
        "            pr = clf.predict(test_data)\n",
        "            res.append(confusion(test_labels,pr))\n",
        "        \n",
        "        print(\"repeat: \",rp,np.mean(res, axis=0).tolist())\n",
        "        overall_result.append(np.mean(res, axis=0).tolist())   \n",
        "    print(\"---------------Result of repeating 10 times for: \",p_center,\"-------------------\")\n",
        "    print(np.mean(np.array(overall_result), axis=0).tolist())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PTuwaLU_0m3Y"
      },
      "outputs": [],
      "source": [
        "crossval_res_kol"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NKil39c20m3Y"
      },
      "outputs": [],
      "source": [
        "results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8PtwJzPJ0m3Z"
      },
      "outputs": [],
      "source": [
        "\n",
        "results.append(test_model(num_epochs = 30))\n",
        "results.append(test_model(num_epochs = 35))\n",
        "results.append(test_model(num_epochs = 40))\n",
        "\n",
        "results"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "celltoolbar": "Tags",
    "colab": {
      "collapsed_sections": [],
      "provenance": [],
      "toc_visible": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}